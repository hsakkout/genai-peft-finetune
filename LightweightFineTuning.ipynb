{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hsakkout/genai-peft-finetune/blob/development/LightweightFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25_vcSBQxZ2D"
      },
      "source": [
        "# Lightweight Fine-Tuning Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJxKmwmXxZ2E"
      },
      "source": [
        "\n",
        "\n",
        "* PEFT technique:  LoRA\n",
        "  *  https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
        "* Model:\n",
        "  * model_id = \"meta-llama/Llama-3.1-8B\"\n",
        "* Evaluation approach:\n",
        "* Fine-tuning dataset:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6suWgDF79emt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# First set the token in Colab's secrets manager\n",
        "# Then access it securely:\n",
        "token = userdata.get('hf_personal_default')\n",
        "\n",
        "# Login using the stored token\n",
        "!huggingface-cli login --token {token}"
      ],
      "metadata": {
        "id": "mG9Upvm-TQSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Google Colab instance installs\n",
        "# Reminder: Restart runtime after installation\n",
        "\n",
        "!pip install peft\n",
        "!pip install bitsandbytes\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install rouge_score\n",
        "!pip install bert_score\n",
        "!pip install wandb\n",
        "\n"
      ],
      "metadata": {
        "id": "YCYUMm3CxiaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujXoj75vxZ2E"
      },
      "source": [
        "## Loading and Evaluating a Foundation Model\n",
        "\n",
        "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsoeWbLAxZ2F"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig  # Let's not use a pipeline, for full transparency\n",
        "import torch\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True\n",
        "        )\n",
        "\n",
        "def setup_model(model_name):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    tokenizer.padding_side = \"left\"\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        pad_token_id=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5dhGQFBxZ2F"
      },
      "outputs": [],
      "source": [
        "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  #  Note: this model needed Meta's license agreement and access approval\n",
        "\n",
        "model, tokenizer = setup_model(model_name)\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate its performance\n"
      ],
      "metadata": {
        "id": "7YjhxQpNIA1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kvlwOKKxH_AN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "\n",
        "class StopOnPeriod(StoppingCriteria):\n",
        "    def __init__(self, tokenizer, stop_id):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.stop_id = stop_id\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        return self.stop_id in input_ids[0][-1:]\n",
        "\n",
        "class StopOnNewline(StoppingCriteria):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.newline_ids = tokenizer.encode('\\n', add_special_tokens=False)\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        n = len(self.newline_ids)\n",
        "        if n > len(input_ids[0]):\n",
        "            return False\n",
        "        if input_ids[0][-n:].tolist() == self.newline_ids:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnPeriod(tokenizer, stop_token_id),StopOnNewline(tokenizer)])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Define bad words to avoid\n",
        "bad_phrases = ['The final', 'The answer', 'The correct', 'Answer:',\n",
        "    'A', 'B', 'A)', 'B)', 'Option A', 'Option B',\n",
        "    'Choice A', 'Choice B', '(A)', '(B)']\n",
        "bad_words_ids = tokenizer(bad_phrases, add_special_tokens=False).input_ids\n",
        "\n",
        "\n",
        "# Get the token ID for the period\n",
        "stop_token_id = tokenizer.encode(\".\", add_special_tokens=False)[-1]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "433Ybx29FWWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_new_tokens=20):\n",
        "    # Prepare inputs\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Move inputs to GPU\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate text\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        stopping_criteria=stopping_criteria,\n",
        "        bad_words_ids=bad_words_ids,\n",
        "        no_repeat_ngram_size=3,\n",
        "        temperature=0.1\n",
        "    )\n",
        "\n",
        "    # Calculate the length of the input prompt\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # Extract only the generated tokens (excluding the prompt)\n",
        "    generated_tokens = outputs[:, input_length:]\n",
        "\n",
        "    # Decode only the generated tokens\n",
        "    generated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Adjusted prompt\n",
        "prompt = (\n",
        "    \"Definition: In each example you will read a short sentence (or two). \"\n",
        "    \"Then, a pronoun in the text is mentioned. Your task is to choose a referent which the mentioned pronoun refers to. \"\n",
        "    \"You will be given two options in each case and one choice should seem much more likely to you than the other.\\n\\n\"\n",
        "    \"Positive Example 1 - Input: sentence: I couldn't put the saucepan on the rack because it was too tall. \"\n",
        "    \"pronoun: it. A) the saucepan B) the rack Output: the rack\\n\\n\"\n",
        "    \"Positive Example 2 - Input: sentence: Arnold greatly influenced Jackson, though he lived two centuries earlier. \"\n",
        "    \"pronoun: he. A) jackson B) arnold Output: arnold\\n\\n\"\n",
        "    \"Negative Example 1 - Input: sentence: Joe and David's uncles can still beat them at tennis, even though they are 30 years younger. \"\n",
        "    \"pronoun: they. A) joe and david B) joe and david's uncles Output: joe and david's uncles\\n\\n\"\n",
        "    \"Negative Example 2 - Input: sentence: Gaston passed the half-empty plate to Hershel because he was full. \"\n",
        "    \"pronoun: he. A) Gaston B) Hershel Output: Hershel\\n\\n\"\n",
        "    \"Now complete the following example - Input: sentence: Joe and Steve paid the detectives after they delivered the final report on the case. \"\n",
        "    \"pronoun: they. A) joe and steve B) the detectives Output:\"\n",
        ")\n",
        "\n",
        "result = generate_text(model, tokenizer, prompt, max_new_tokens=10)\n",
        "\n",
        "print(result[0].strip())"
      ],
      "metadata": {
        "id": "KI4x2ndLwdEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\n",
        "    'Lots-of-LoRAs/task249_enhanced_wsc_pronoun_disambiguation',\n",
        "    split='train'\n",
        ")\n",
        "\n",
        "\n",
        "# Print the column names\n",
        "print(\"Dataset Columns:\", dataset.column_names)"
      ],
      "metadata": {
        "id": "rYIMpRuJFWTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Adjust the preprocessing function based on actual column names\n",
        "def preprocess_function(examples):\n",
        "    inputs = examples['input']\n",
        "    raw_references = examples['output']\n",
        "\n",
        "    prompts = []\n",
        "    references = []\n",
        "\n",
        "    for input_text, ref in zip(inputs, raw_references):\n",
        "        # Build the prompt for each example\n",
        "        prompt = (\n",
        "            input_text.strip()\n",
        "        )\n",
        "        prompts.append(prompt)\n",
        "\n",
        "        # Process the reference\n",
        "        if isinstance(ref, list):\n",
        "            ref = ref[0] if ref else ''\n",
        "        else:\n",
        "            ref = str(ref)\n",
        "\n",
        "        references.append(ref.strip())\n",
        "\n",
        "    return {\n",
        "        'prompts': prompts,\n",
        "        'references': references,\n",
        "    }\n",
        "\n",
        "data = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Limit to the first example for testing (optional)\n",
        "data = data.select(range(20))\n",
        "\n",
        "# Step 4: Generate model outputs in batches\n",
        "batch_size = 10  # Adjust based on your GPU memory\n",
        "generated_answers = []\n",
        "references = []\n",
        "questions_list = []\n",
        "\n",
        "for i in tqdm(range(0, len(data), batch_size)):\n",
        "    batch = data[i:i+batch_size]\n",
        "    prompts = batch['prompts']\n",
        "    refs = batch['references']\n",
        "\n",
        "    # Tokenize inputs\n",
        "    inputs = tokenizer(\n",
        "        prompts,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=400\n",
        "    )\n",
        "\n",
        "    # Generate outputs\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=15,  # Adjust as needed\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        stopping_criteria=stopping_criteria,\n",
        "        bad_words_ids=bad_words_ids,\n",
        "        no_repeat_ngram_size=3,\n",
        "        temperature=0.1,\n",
        "        num_beams=5,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    # **Calculate the length of the input prompt**\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "\n",
        "    # **Extract only the generated tokens (excluding the prompt)**\n",
        "    generated_tokens = outputs[:, input_length:]\n",
        "\n",
        "    # Decode outputs\n",
        "    decoded_outputs = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    # Post-process outputs if necessary\n",
        "    processed_outputs = [output.strip() for output in decoded_outputs]\n",
        "\n",
        "    generated_answers.extend(processed_outputs)\n",
        "    references.extend(refs)\n",
        "    questions_list.extend(prompts)\n",
        "\n",
        "    # Clear CUDA cache\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Minimal post-processing to clean the outputs (optional)\n",
        "cleaned_outputs = []\n",
        "for output in generated_answers:\n",
        "    # Split at the first newline or period\n",
        "    output = output.split('\\n')[0].split('.')[0].strip()\n",
        "    cleaned_outputs.append(output)\n",
        "\n",
        "# Step 5: Compute evaluation metrics and store detailed results\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "detailed_results = []\n",
        "\n",
        "for idx, (question, ref, pred) in enumerate(zip(\n",
        "    questions_list, references, cleaned_outputs)):\n",
        "    # Compute ROUGE scores for our model's prediction\n",
        "    scores = scorer.score(ref, pred)\n",
        "\n",
        "    # Store the detailed results\n",
        "    detailed_results.append({\n",
        "        'No.': idx + 1,\n",
        "        'Question': question,\n",
        "        'Reference': ref,\n",
        "        'Our Prediction': pred,\n",
        "        'Our ROUGE-1 F1': scores['rouge1'].fmeasure,\n",
        "        'Our ROUGE-2 F1': scores['rouge2'].fmeasure,\n",
        "        'Our ROUGE-L F1': scores['rougeL'].fmeasure,\n",
        "    })\n",
        "\n",
        "# Step 6: Create a pandas DataFrame\n",
        "df = pd.DataFrame(detailed_results)\n",
        "\n",
        "# Set pandas options for better display\n",
        "pd.set_option('display.max_colwidth', None)  # Don't truncate text in cells\n",
        "\n",
        "# Display the DataFrame in the notebook\n",
        "display(df[['No.', 'Question', 'Reference', 'Our Prediction',\n",
        "            'Our ROUGE-1 F1', 'Our ROUGE-2 F1', 'Our ROUGE-L F1']])\n",
        "\n",
        "# Also print the average scores\n",
        "avg_rouge1 = df['Our ROUGE-1 F1'].mean()\n",
        "avg_rouge2 = df['Our ROUGE-2 F1'].mean()\n",
        "avg_rougeL = df['Our ROUGE-L F1'].mean()\n",
        "\n",
        "print(f\"\\nAverage ROUGE Scores for Our Model:\")\n",
        "print(f\"ROUGE-1 F1 Score: {avg_rouge1:.4f}\")\n",
        "print(f\"ROUGE-2 F1 Score: {avg_rouge2:.4f}\")\n",
        "print(f\"ROUGE-L F1 Score: {avg_rougeL:.4f}\")"
      ],
      "metadata": {
        "id": "NSf73IfOtkFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LLxqnHO5utA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKHFGFF8xZ2F"
      },
      "source": [
        "## Performing Parameter-Efficient Fine-Tuning\n",
        "\n",
        "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# config = LoraConfig(\n",
        "#     task_type=\"CAUSAL_LM\",\n",
        "#     inference_mode=False,\n",
        "#     r=16,\n",
        "#     lora_alpha=50,\n",
        "#     use_rslora=True,   # automatically sets adapting scaling factor based on lora_alpha to lora_alpha/math.sqrt(r), shd be better than original default\n",
        "#     layers_to_transform=list(range(24,32)),\n",
        "#     lora_dropout=0.05,\n",
        "#     bias=\"none\",\n",
        "#     target_modules=[ # Llama 3.1-8B target modules\n",
        "#         \"q_proj\",      # 4096 -> 4096\n",
        "#         \"k_proj\",      # 4096 -> 1024\n",
        "#         \"v_proj\",      # 4096 -> 1024\n",
        "#         \"gate_proj\",   # 4096 -> 14336\n",
        "#         \"up_proj\",     # 4096 -> 14336\n",
        "#     ],\n",
        "# )"
      ],
      "metadata": {
        "id": "51tdRNTIEzJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZyffZJqxZ2F"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import (\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA configuration\n",
        "config = LoraConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    inference_mode=False,\n",
        "    r=16,\n",
        "    lora_alpha=50,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\"],\n",
        "    # Note: Applying LoRA to specific layers may require custom code\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)\n",
        "\n",
        "# Optionally, print the trainable parameters to verify\n",
        "lora_model.print_trainable_parameters()\n",
        "\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Filter out any examples with empty prompts or references\n",
        "data = data.filter(lambda example: example['prompts'] and example['references'])\n",
        "\n",
        "# # Split the dataset if needed (e.g., train/test split)\n",
        "# # For example, if your dataset has a 'train' split:\n",
        "# data = data['train']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Concatenate prompts and references\n",
        "    inputs = [prompt.strip() + ' ' + reference.strip() + tokenizer.eos_token for prompt, reference in zip(examples['prompts'], examples['references'])]\n",
        "\n",
        "    # Tokenize the concatenated texts\n",
        "    tokenized_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=512,  # Adjust as needed\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "    )\n",
        "\n",
        "    # Now, create labels by masking the prompt tokens\n",
        "    input_ids = tokenized_inputs['input_ids']\n",
        "    labels = []\n",
        "\n",
        "    for input_id, prompt in zip(input_ids, examples['prompts']):\n",
        "        # Tokenize the prompt\n",
        "        prompt_tokenized = tokenizer(\n",
        "            prompt.strip(),\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        prompt_length = len(prompt_tokenized['input_ids'])\n",
        "\n",
        "        # Create labels, masking the prompt tokens\n",
        "        labels_ids = [-100] * prompt_length + input_id[prompt_length:]\n",
        "        labels.append(labels_ids)\n",
        "\n",
        "    # Ensure labels are the same length as input_ids\n",
        "    for i in range(len(labels)):\n",
        "        if len(labels[i]) < len(input_ids[i]):\n",
        "            labels[i] += [-100] * (len(input_ids[i]) - len(labels[i]))\n",
        "        elif len(labels[i]) > len(input_ids[i]):\n",
        "            labels[i] = labels[i][:len(input_ids[i])]\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids,\n",
        "        'attention_mask': tokenized_inputs['attention_mask'],\n",
        "        'labels': labels,\n",
        "    }\n",
        "\n",
        "# Apply the tokenization function\n",
        "tokenized_datasets = data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=data.column_names,\n",
        ")\n",
        "\n",
        "\n",
        "# Define a custom data collator to handle dynamic padding\n",
        "def data_collator(features):\n",
        "    batch = {}\n",
        "\n",
        "    # Get max sequence length in the batch\n",
        "    max_length = max(len(f['input_ids']) for f in features)\n",
        "\n",
        "    # Pad input_ids and labels\n",
        "    input_ids = []\n",
        "    attention_mask = []\n",
        "    labels = []\n",
        "    for f in features:\n",
        "        pad_length = max_length - len(f['input_ids'])\n",
        "        input_ids.append(f['input_ids'] + [tokenizer.pad_token_id] * pad_length)\n",
        "        attention_mask.append([1] * len(f['input_ids']) + [0] * pad_length)\n",
        "        labels.append(f['labels'] + [-100] * pad_length)\n",
        "\n",
        "    batch['input_ids'] = torch.tensor(input_ids, dtype=torch.long)\n",
        "    batch['attention_mask'] = torch.tensor(attention_mask, dtype=torch.long)\n",
        "    batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    return batch\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=1,  # Adjust based on your GPU memory\n",
        "    gradient_accumulation_steps=16,  # To simulate a larger batch size\n",
        "    evaluation_strategy='no',\n",
        "    save_strategy='epoch',\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,  # Enable mixed-precision training\n",
        "    learning_rate=2e-4,\n",
        "    max_grad_norm=0.3,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    report_to='none',  # Set to 'wandb' or 'tensorboard' if you use them\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "\n",
        "# Clear CUDA cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Start training\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kf_G0llPxZ2F"
      },
      "outputs": [],
      "source": [
        "lora_model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCU6JlLBxZ2F"
      },
      "outputs": [],
      "source": [
        "def print_model_device(model, name=\"model\"):\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"{name} is on device: {device}\")\n",
        "\n",
        "# After loading the model\n",
        "print_model_device(model, \"Base model\")\n",
        "print_model_device(model, \"Model after prepare_model_for_kbit_training\")\n",
        "\n",
        "\n",
        "print_model_device(lora_model, \"LoRA model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABSziih7xZ2F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szSX6m9kxZ2F"
      },
      "outputs": [],
      "source": [
        "lora_model.save_pretrained(\"llama3_1_8b-lora-pretraining\")\n",
        "# lora_model.save_pretrained(\"llama3_1_8b-lora-posttraining\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wYZK7ucmDj5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwbWWcx1xZ2F"
      },
      "source": [
        "## Performing Inference with a PEFT Model\n",
        "\n",
        "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_Rz7JQexZ2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exSCyAKrxZ2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXmDCupsxZ2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYNw7h8lxZ2G"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y7QrwtgxZ2G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}