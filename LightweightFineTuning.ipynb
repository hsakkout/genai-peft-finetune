{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hsakkout/genai-peft-finetune/blob/development/LightweightFineTuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25_vcSBQxZ2D"
   },
   "source": [
    "# Lightweight LLAMA 3 Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJxKmwmXxZ2E"
   },
   "source": [
    "\n",
    "### Overview\n",
    "\n",
    "* PEFT technique:  [**LoRA**](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "* Model: [**meta-llama/Llama-3.1-8B**](https://huggingface.co/meta-llama/Llama-3.1-8B)\n",
    "* Evaluation approach: **Pronoun disambiguation enhancement (an LLM dynamic text classification problem). [Rouge Scoring](https://huggingface.co/spaces/evaluate-metric/rouge)**\n",
    "* Fine-tuning dataset:  [**Lots-of-LoRAs/task249_enhanced_wsc_pronoun_disambiguation**](https://huggingface.co/datasets/Lots-of-LoRAs/task249_enhanced_wsc_pronoun_disambiguation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1LSlG6cp7Of"
   },
   "source": [
    "### Setup Google Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6suWgDF79emt",
    "outputId": "c8f50c0b-1c79-40c4-f5dd-6b158a7b0a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mG9Upvm-TQSf",
    "outputId": "251bb8cf-382a-4963-89d7-8a270e31b3e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from google.colab import userdata\n",
    "\n",
    "# First set the token in Colab's secrets manager\n",
    "# Then access it securely:\n",
    "token = userdata.get('hf_personal_default')\n",
    "\n",
    "# Login using the stored token\n",
    "!huggingface-cli login --token {token}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YCYUMm3CxiaX",
    "outputId": "6643e19a-7005-4692-a5f6-229395830c86"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
      "Downloading peft-0.13.2-py3-none-any.whl (320 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: peft\n",
      "Successfully installed peft-0.13.2\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bitsandbytes\n",
      "Successfully installed bitsandbytes-0.44.1\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
      "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (16.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge_score) (4.66.5)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=bd0c796c0e4d8725ead9d9141a5073a39ba3573a22d0cb35cdfa2eab48f2d047\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n",
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.5.0+cu121)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.44.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert_score) (1.26.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert_score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert_score) (4.66.5)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert_score) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert_score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert_score) (2024.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (2024.6.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.24.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=3.0.0->bert_score) (0.19.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert_score) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert_score) (2024.8.30)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n",
      "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.18.5)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.17.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.8.30)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Google Colab instance installs\n",
    "# Reminder: Restart runtime after installation\n",
    "\n",
    "!pip install peft\n",
    "!pip install bitsandbytes\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "!pip install rouge_score\n",
    "!pip install bert_score\n",
    "!pip install wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujXoj75vxZ2E"
   },
   "source": [
    "## Load and Evaluate a Foundation Model\n",
    "\n",
    "- Choose and load a pre-trained Hugging Face model\n",
    "- Load a data set for evaluation\n",
    "- Evaluate its performance prior to Parameter-*Efficient* Fine-Tuning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PgRENorrpvQi"
   },
   "source": [
    "### Choose and Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "TsoeWbLAxZ2F"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, set_seed\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Random seeds for reproducibility\n",
    "SEED = 42\n",
    "\n",
    "set_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# CUDA seeds\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure operations deterministic on GPU\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "PyFZmq3gYchG"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Due to the size of the base LLM, applying quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "\n",
    "def setup_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518,
     "referenced_widgets": [
      "61191cd3c85048e2a5d43911ae71273a",
      "d6967496e9244a089ca6490ba4962307",
      "45d4f5e05ec0439294e7a7e298325d87",
      "367a36f7ed494312bfb4014900717580",
      "d528a17a42744b60a5ac5e187624b488",
      "adbef3527b3c4a28b49352332303253e",
      "07aef3fb740f4b83a8bb42435da8f0a5",
      "10338e3da879478d81a2a14035efd1ce",
      "817b866a3c2e41909343462ef4f016a3",
      "3eb81cd6d19d438e80de6b3faf101c5e",
      "0b3fc1514c5544ea900d9555b6216a49"
     ]
    },
    "id": "X5dhGQFBxZ2F",
    "outputId": "66739c78-c6af-46cb-ca7c-0bfc3868ec70"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61191cd3c85048e2a5d43911ae71273a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128009)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Llama 3.1 is a powerful foundational model, while the 8bn parameter variant is still of manageable size\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"  #  Note: use of this model is subject to Meta's license agreement and access approval\n",
    "\n",
    "model, tokenizer = setup_model(model_name)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7YjhxQpNIA1p"
   },
   "source": [
    "### Evaluate its performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hj57BtsTosVL"
   },
   "source": [
    "#### Configure the generation to make the answers less verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "433Ybx29FWWC"
   },
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "# I will evaluate Llama 3.1 on a dynamic classification problem, where the classes need to be stated by the LLM succinctly.\n",
    "# The following aims to reduce the verbosity of the response from Llama 3.1.  Without configuration, it tends to produce long, chatty responses.\n",
    "\n",
    "class StopOnPeriod(StoppingCriteria):\n",
    "    def __init__(self, tokenizer, stop_id):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stop_id = stop_id\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return self.stop_id in input_ids[0][-1:]\n",
    "\n",
    "class StopOnNewline(StoppingCriteria):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.newline_ids = tokenizer.encode('\\n', add_special_tokens=False)\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        n = len(self.newline_ids)\n",
    "        if n > len(input_ids[0]):\n",
    "            return False\n",
    "        if input_ids[0][-n:].tolist() == self.newline_ids:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "\n",
    "stop_token_id = tokenizer.encode(\".\", add_special_tokens=False)[-1]\n",
    "stopping_criteria = StoppingCriteriaList([StopOnPeriod(tokenizer, stop_token_id),StopOnNewline(tokenizer)])\n",
    "\n",
    "\n",
    "# Define bad words to avoid, again to reduce chatter\n",
    "bad_phrases = ['The final', 'The answer', 'The correct', 'Answer:',\n",
    "    'A', 'B', 'A)', 'B)', 'Option A', 'Option B',\n",
    "    'Choice A', 'Choice B', '(A)', '(B)']\n",
    "bad_words_ids = tokenizer(bad_phrases, add_special_tokens=False).input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48svB5p3pVtV"
   },
   "source": [
    "#### Define a method for interactive experimentation with the configured foundational model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KI4x2ndLwdEM",
    "outputId": "63330293-b3b5-4837-af50-4ead5ce3f091"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: \n",
      "[' \\nAnswer:\\nthe detectives\\n']\n",
      "\n",
      "\n",
      "STRIPPED RESULT: \n",
      "Answer:\n",
      "the detectives\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This method is for ad hoc text generation, to experiment interactively with the configured model's outputs.\n",
    "\n",
    "\n",
    "def generate_text(model_in, tokenizer_in, prompt, max_new_tokens=20):\n",
    "    # Prepare inputs\n",
    "    inputs = tokenizer_in(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    # Move inputs to GPU\n",
    "    inputs = {k: v.to(model_in.device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate text\n",
    "    outputs = model_in.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        eos_token_id=tokenizer_in.eos_token_id,\n",
    "        pad_token_id=tokenizer_in.pad_token_id,\n",
    "        stopping_criteria=stopping_criteria,\n",
    "        bad_words_ids=bad_words_ids,\n",
    "        no_repeat_ngram_size=3,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "    # Calculate the length of the input prompt\n",
    "    input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "    # Slice only the generated tokens (excluding the prompt) - this Llama 3.1 model returns the combined prompt and answer.\n",
    "    generated_tokens = outputs[:, input_length:]\n",
    "\n",
    "    # Decode only the generated tokens\n",
    "    generated_text = tokenizer_in.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "# Adjusted prompt\n",
    "prompt = (\n",
    "    \"Definition: In each example you will read a short sentence (or two). \"\n",
    "    \"Then, a pronoun in the text is mentioned. Your task is to choose a referent which the mentioned pronoun refers to. \"\n",
    "    \"You will be given two options in each case and one choice should seem much more likely to you than the other.\\n\\n\"\n",
    "    \"Positive Example 1 - Input: sentence: I couldn't put the saucepan on the rack because it was too tall. \"\n",
    "    \"pronoun: it. A) the saucepan B) the rack Output: the rack\\n\\n\"\n",
    "    \"Positive Example 2 - Input: sentence: Arnold greatly influenced Jackson, though he lived two centuries earlier. \"\n",
    "    \"pronoun: he. A) jackson B) arnold Output: arnold\\n\\n\"\n",
    "    \"Negative Example 1 - Input: sentence: Joe and David's uncles can still beat them at tennis, even though they are 30 years younger. \"\n",
    "    \"pronoun: they. A) joe and david B) joe and david's uncles Output: joe and david's uncles\\n\\n\"\n",
    "    \"Negative Example 2 - Input: sentence: Gaston passed the half-empty plate to Hershel because he was full. \"\n",
    "    \"pronoun: he. A) Gaston B) Hershel Output: Hershel\\n\\n\"\n",
    "    \"Now complete the following example - Input: sentence: Joe and Steve paid the detectives after they delivered the final report on the case. \"\n",
    "    \"pronoun: they. A) joe and steve B) the detectives Output:\"\n",
    ")\n",
    "\n",
    "result = generate_text(model, tokenizer, prompt, max_new_tokens=10)\n",
    "\n",
    "print(f'RESULT: \\n{result}\\n\\n')\n",
    "print(f'STRIPPED RESULT: \\n{result[0].strip()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-9gJd7QqPvP"
   },
   "source": [
    "#### Load the two datasets (train and test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYIMpRuJFWTZ",
    "outputId": "bca80f6c-f2c7-4e83-aaf2-31361a91e329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Columns: ['input', 'output', 'id']\n",
      "Train Dataset Rows : 547\n",
      "Test Dataset Columns: ['input', 'output', 'id']\n",
      "Test Dataset Rows : 69\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets for more rigorous evaluation\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "train_dataset = load_dataset(\n",
    "    'Lots-of-LoRAs/task249_enhanced_wsc_pronoun_disambiguation',\n",
    "    split='train'\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "test_dataset = load_dataset(\n",
    "    'Lots-of-LoRAs/task249_enhanced_wsc_pronoun_disambiguation',\n",
    "    split='test'\n",
    ")\n",
    "\n",
    "# Print the column names\n",
    "print(f'Train Dataset Columns: {train_dataset.column_names}')\n",
    "print(f'Train Dataset Rows : {len(train_dataset)}')\n",
    "print(f'Test Dataset Columns: {test_dataset.column_names}')\n",
    "print(f'Test Dataset Rows : {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHUh9H4BqgPp"
   },
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "NSf73IfOtkFn"
   },
   "outputs": [],
   "source": [
    "# Preprocess raw data set inputs into local 'prompts' and 'references' columns\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples['input']\n",
    "    raw_references = examples['output']\n",
    "\n",
    "    prompts = []\n",
    "    references = []\n",
    "\n",
    "    for input_text, ref in zip(inputs, raw_references):\n",
    "        # Build the prompt for each example\n",
    "        prompt = (\n",
    "            input_text.strip()\n",
    "        )\n",
    "        prompts.append(prompt)\n",
    "\n",
    "        # Process the reference\n",
    "        if isinstance(ref, list):\n",
    "            ref = ref[0] if ref else ''\n",
    "        else:\n",
    "            ref = str(ref)\n",
    "\n",
    "        references.append(ref.strip())\n",
    "\n",
    "    return {\n",
    "        'prompts': prompts,\n",
    "        'references': references,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dfjejhcqrTc"
   },
   "source": [
    "#### Method for Systematic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "ZLOQ-ss7feeA"
   },
   "outputs": [],
   "source": [
    "# Systematic evaluation method\n",
    "\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate_quality(rawdataset, model_in, tokenizer_in):\n",
    "    dest_device = model_in.device\n",
    "\n",
    "    data = rawdataset.map(preprocess_function, batched=True)\n",
    "    # Generate model outputs in batches\n",
    "    batch_size = 5  # Adjust based on GPU memory\n",
    "    generated_answers = []\n",
    "    references = []\n",
    "    questions_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(data), batch_size)):\n",
    "        batch = data[i:i+batch_size]\n",
    "        prompts = batch['prompts']\n",
    "        refs = batch['references']\n",
    "\n",
    "        # Clear CUDA cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer_in(\n",
    "            prompts,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=400\n",
    "        )\n",
    "\n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(dest_device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate outputs\n",
    "        outputs = model_in.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=15,  # Adjust as needed\n",
    "            eos_token_id=tokenizer_in.eos_token_id,\n",
    "            pad_token_id=tokenizer_in.pad_token_id,\n",
    "            stopping_criteria=stopping_criteria,\n",
    "            bad_words_ids=bad_words_ids,\n",
    "            no_repeat_ngram_size=3,\n",
    "            temperature=0.1,\n",
    "            num_beams=5,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "        # **Calculate the length of the input prompt**\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        # **Extract only the generated tokens (excluding the prompt)**\n",
    "        generated_tokens = outputs[:, input_length:]\n",
    "\n",
    "        # Decode outputs\n",
    "        decoded_outputs = tokenizer_in.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process outputs if necessary\n",
    "        processed_outputs = [output.strip() for output in decoded_outputs]\n",
    "\n",
    "        generated_answers.extend(processed_outputs)\n",
    "        references.extend(refs)\n",
    "        questions_list.extend(prompts)\n",
    "\n",
    "\n",
    "    # Minimal post-processing to clean the outputs (optional)\n",
    "    cleaned_outputs = []\n",
    "    for output in generated_answers:\n",
    "        # Split at the first newline or period\n",
    "        output = output.split('\\n')[0].split('.')[0].strip()\n",
    "        cleaned_outputs.append(output)\n",
    "\n",
    "    # Step 5: Compute evaluation metrics and store detailed results\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    detailed_results = []\n",
    "\n",
    "    for idx, (question, ref, pred) in enumerate(zip(\n",
    "        questions_list, references, cleaned_outputs)):\n",
    "        # Compute ROUGE scores for our model's prediction\n",
    "        scores = scorer.score(ref, pred)\n",
    "\n",
    "        # Store the detailed results\n",
    "        detailed_results.append({\n",
    "            'No.': idx + 1,\n",
    "            'Question': question,\n",
    "            'Abbrv Question': question[-300:],\n",
    "            'Reference': ref,\n",
    "            'Our Prediction': pred,\n",
    "            'Our ROUGE-1 F1': scores['rouge1'].fmeasure,\n",
    "            'Our ROUGE-2 F1': scores['rouge2'].fmeasure,\n",
    "            'Our ROUGE-L F1': scores['rougeL'].fmeasure,\n",
    "        })\n",
    "\n",
    "    # Step 6: Create a pandas DataFrame\n",
    "    df = pd.DataFrame(detailed_results)\n",
    "\n",
    "    # Set pandas options for better display\n",
    "    pd.set_option('display.max_colwidth', None)  # Don't truncate text in cells\n",
    "\n",
    "    # Display the DataFrame in the notebook\n",
    "    display(df[['No.', 'Abbrv Question', 'Reference', 'Our Prediction',\n",
    "                'Our ROUGE-1 F1', 'Our ROUGE-2 F1', 'Our ROUGE-L F1']])\n",
    "\n",
    "    # Also print the average scores\n",
    "    avg_rouge1 = df['Our ROUGE-1 F1'].mean()\n",
    "    avg_rouge2 = df['Our ROUGE-2 F1'].mean()\n",
    "    avg_rougeL = df['Our ROUGE-L F1'].mean()\n",
    "\n",
    "    print(f\"\\nAverage ROUGE Scores for Our Model:\")\n",
    "    print(f\"ROUGE-1 F1 Score: {avg_rouge1:.4f}\")\n",
    "    print(f\"ROUGE-2 F1 Score: {avg_rouge2:.4f}\")\n",
    "    print(f\"ROUGE-L F1 Score: {avg_rougeL:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "RqhfN4SffnTL",
    "outputId": "08a1b3d0-0d60-429b-c9fe-64cd1e41890d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:35<00:00,  2.51s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"evaluate_quality(test_dataset, model, tokenizer)\",\n  \"rows\": 69,\n  \"fields\": [\n    {\n      \"column\": \"No.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 1,\n        \"max\": 69,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          23,\n          1,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abbrv Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 69,\n        \"samples\": [\n          \" .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Dr . Adams and Dr . Jones informed Kate and Laura that they had retired and presented several options for future treatment .pronoun: they. A) dr . adams and dr . jones B) kate and laura\\nOutput:\",\n          \"e: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: freddy was upset with travis because the toaster he had bought from him didn't work .pronoun: he. A) freddy B) travis\\nOutput:\",\n          \"full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: George and Bob got free tickets to the play , but they gave them to Eric and Jeff , because they were particularly eager to see it .pronoun: they. A) george and bob B) eric and jeff\\nOutput:\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 66,\n        \"samples\": [\n          \"the large balls\",\n          \"susan and hannah\",\n          \"freddy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 47,\n        \"samples\": [\n          \"sALLY AND MARY\",\n          \"martin and pual\",\n          \"The bystander\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our ROUGE-1 F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43363012121603756,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.33333333333333337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our ROUGE-2 F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27066858719876186,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.6666666666666666,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our ROUGE-L F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.43363012121603756,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.3333333333333333,\n          1.0,\n          0.33333333333333337\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-94bcdb5b-1655-4fe9-891c-57d5a1964935\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Abbrv Question</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Our Prediction</th>\n",
       "      <th>Our ROUGE-1 F1</th>\n",
       "      <th>Our ROUGE-2 F1</th>\n",
       "      <th>Our ROUGE-L F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: freddy was upset with travis because the toaster he had bought from him didn't work .pronoun: he. A) freddy B) travis\\nOutput:</td>\n",
       "      <td>freddy</td>\n",
       "      <td>travis</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>te to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Fred covered his eye with his hand , because the wind was blowing sand around . He lowered it when the wind stopped .pronoun: it. A) his eye B) his hand\\nOutput:</td>\n",
       "      <td>his hand</td>\n",
       "      <td>his hand</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ut: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: luke couldn't see the stage with adam in front of him because he is so short .pronoun: he. A) luke B) adam\\nOutput:</td>\n",
       "      <td>luke</td>\n",
       "      <td>adam</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>f-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: the board of aldermen refused the protesters a permit because they feared violence .pronoun: they. A) the board of aldermen B) the protesters\\nOutput:</td>\n",
       "      <td>the board of aldermen</td>\n",
       "      <td>A</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: The man held the boy against his will .pronoun: his. A) the man B) the boy\\nOutput:</td>\n",
       "      <td>the boy</td>\n",
       "      <td>A) The man</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>e Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Bill did not pass the ball to Steven although he was open .pronoun: he. A) bill B) steven\\nOutput:</td>\n",
       "      <td>steven</td>\n",
       "      <td>steven</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Susan and Hannah knew that Ann and Polly's sons had been in car accidents , so they told them about it .pronoun: they. A) susan and hannah B) ann and polly\\nOutput:</td>\n",
       "      <td>susan and hannah</td>\n",
       "      <td>susan</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>aston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Peter did a lot worse than his good friend Jeff on the test because he had studied so hard .pronoun: he. A) peter B) jeff\\nOutput:</td>\n",
       "      <td>jeff</td>\n",
       "      <td>jeff</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: I put the heavy books on the tables and they broke .pronoun: they. A) the heavy books B) the tables\\nOutput:</td>\n",
       "      <td>the tables</td>\n",
       "      <td>B</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>david's uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: kirk hired andy to take care of him .pronoun: him. A) kirk B) andy\\nOutput:</td>\n",
       "      <td>kirk</td>\n",
       "      <td>andy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 7 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94bcdb5b-1655-4fe9-891c-57d5a1964935')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-94bcdb5b-1655-4fe9-891c-57d5a1964935 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-94bcdb5b-1655-4fe9-891c-57d5a1964935');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-eb29c0e2-056c-40b8-ac24-62d56333a226\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eb29c0e2-056c-40b8-ac24-62d56333a226')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-eb29c0e2-056c-40b8-ac24-62d56333a226 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    No.  \\\n",
       "0     1   \n",
       "1     2   \n",
       "2     3   \n",
       "3     4   \n",
       "4     5   \n",
       "..  ...   \n",
       "64   65   \n",
       "65   66   \n",
       "66   67   \n",
       "67   68   \n",
       "68   69   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          Abbrv Question  \\\n",
       "0      e: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: freddy was upset with travis because the toaster he had bought from him didn't work .pronoun: he. A) freddy B) travis\\nOutput:   \n",
       "1      te to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Fred covered his eye with his hand , because the wind was blowing sand around . He lowered it when the wind stopped .pronoun: it. A) his eye B) his hand\\nOutput:   \n",
       "2      ut: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: luke couldn't see the stage with adam in front of him because he is so short .pronoun: he. A) luke B) adam\\nOutput:   \n",
       "3      f-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: the board of aldermen refused the protesters a permit because they feared violence .pronoun: they. A) the board of aldermen B) the protesters\\nOutput:   \n",
       "4   uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: The man held the boy against his will .pronoun: his. A) the man B) the boy\\nOutput:   \n",
       "..                                                                                                                                                                                                                                                                                                                   ...   \n",
       "64    e Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Bill did not pass the ball to Steven although he was open .pronoun: he. A) bill B) steven\\nOutput:   \n",
       "65     to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Susan and Hannah knew that Ann and Polly's sons had been in car accidents , so they told them about it .pronoun: they. A) susan and hannah B) ann and polly\\nOutput:   \n",
       "66     aston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Peter did a lot worse than his good friend Jeff on the test because he had studied so hard .pronoun: he. A) peter B) jeff\\nOutput:   \n",
       "67    2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: I put the heavy books on the tables and they broke .pronoun: they. A) the heavy books B) the tables\\nOutput:   \n",
       "68  david's uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: kirk hired andy to take care of him .pronoun: him. A) kirk B) andy\\nOutput:   \n",
       "\n",
       "                Reference Our Prediction  Our ROUGE-1 F1  Our ROUGE-2 F1  \\\n",
       "0                  freddy         travis             0.0             0.0   \n",
       "1                his hand       his hand             1.0             1.0   \n",
       "2                    luke           adam             0.0             0.0   \n",
       "3   the board of aldermen              A             0.0             0.0   \n",
       "4                 the boy     A) The man             0.4             0.0   \n",
       "..                    ...            ...             ...             ...   \n",
       "64                 steven         steven             1.0             0.0   \n",
       "65       susan and hannah          susan             0.5             0.0   \n",
       "66                   jeff           jeff             1.0             0.0   \n",
       "67             the tables              B             0.0             0.0   \n",
       "68                   kirk           andy             0.0             0.0   \n",
       "\n",
       "    Our ROUGE-L F1  \n",
       "0              0.0  \n",
       "1              1.0  \n",
       "2              0.0  \n",
       "3              0.0  \n",
       "4              0.4  \n",
       "..             ...  \n",
       "64             1.0  \n",
       "65             0.5  \n",
       "66             1.0  \n",
       "67             0.0  \n",
       "68             0.0  \n",
       "\n",
       "[69 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores for Our Model:\n",
      "ROUGE-1 F1 Score: 0.3619\n",
      "ROUGE-2 F1 Score: 0.0848\n",
      "ROUGE-L F1 Score: 0.3619\n"
     ]
    }
   ],
   "source": [
    "\n",
    "evaluate_quality(test_dataset, model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKHFGFF8xZ2F"
   },
   "source": [
    "## Perform Parameter-Efficient Fine-Tuning\n",
    "\n",
    "- Create a PEFT model from the loaded model\n",
    "- Run a training loop\n",
    "- Save the PEFT model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jixluosZsBHd"
   },
   "source": [
    "### Setup up Trainable LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZyffZJqxZ2F",
    "outputId": "b0fe88f8-ea52-4bb4-9d1b-f8c25be62be4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 28,311,552 || all params: 8,058,572,800 || trainable%: 0.3513\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Prepare the model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Apply LoRA configuration\n",
    "config = LoraConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    inference_mode=False,\n",
    "    r=16,\n",
    "    lora_alpha=50,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\"],\n",
    "    # Note: Applying LoRA to specific layers may require custom code\n",
    ")\n",
    "\n",
    "lora_model = get_peft_model(model, config)\n",
    "\n",
    "# Optionally, print the trainable parameters to verify\n",
    "lora_model.print_trainable_parameters()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcLnnW-auBB8"
   },
   "source": [
    "### Train the LoRA Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMufYx3CuS4o"
   },
   "source": [
    "#### Prepare and handle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ItzaWCLer-tn"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_data = train_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Filter out any examples with empty prompts or references\n",
    "train_data = train_data.filter(lambda example: example['prompts'] and example['references'])\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize prompts and references separately\n",
    "    tokenized_prompts = tokenizer(\n",
    "        examples['prompts'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    tokenized_references = tokenizer(\n",
    "        examples['references'],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "\n",
    "    input_ids = []\n",
    "    labels = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for prompt_ids, reference_ids in zip(tokenized_prompts['input_ids'], tokenized_references['input_ids']):\n",
    "        # Concatenate prompt and reference IDs\n",
    "        input_id = prompt_ids + reference_ids + [tokenizer.eos_token_id]\n",
    "        input_ids.append(input_id)\n",
    "\n",
    "        # Create labels by masking the prompt tokens\n",
    "        label = [-100] * len(prompt_ids) + reference_ids + [tokenizer.eos_token_id]\n",
    "        labels.append(label)\n",
    "\n",
    "        # Create attention mask\n",
    "        attention_mask = [1] * len(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'labels': labels,\n",
    "    }\n",
    "\n",
    "# Apply the tokenization function\n",
    "tokenized_datasets = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "\n",
    "# Define a custom data collator to handle dynamic padding\n",
    "def data_collator(features):\n",
    "    batch = {}\n",
    "\n",
    "    # Get max sequence length in the batch\n",
    "    max_length = max(len(f['input_ids']) for f in features)\n",
    "\n",
    "    # Pad input_ids and labels\n",
    "    input_ids = []\n",
    "    attention_mask = []\n",
    "    labels = []\n",
    "    for f in features:\n",
    "        pad_length = max_length - len(f['input_ids'])\n",
    "        input_ids.append(f['input_ids'] + [tokenizer.pad_token_id] * pad_length)\n",
    "        attention_mask.append([1] * len(f['input_ids']) + [0] * pad_length)\n",
    "        labels.append(f['labels'] + [-100] * pad_length)\n",
    "\n",
    "    batch['input_ids'] = torch.tensor(input_ids, dtype=torch.long)\n",
    "    batch['attention_mask'] = torch.tensor(attention_mask, dtype=torch.long)\n",
    "    batch['labels'] = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ou0df-ECuOt5"
   },
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 721
    },
    "id": "m4BSIEYnuM52",
    "outputId": "c8a13cb2-5c43-4624-c630-ebc33c4e86e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 04:34, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.410500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.137200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.131000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.123400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at ../aten/src/ATen/native/cudnn/MHA.cpp:674.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=1.3457130424532235, metrics={'train_runtime': 276.7183, 'train_samples_per_second': 5.93, 'train_steps_per_second': 0.369, 'total_flos': 2.308054007370547e+16, 'train_loss': 1.3457130424532235, 'epoch': 2.978102189781022})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Result of experimentation\n",
    "    gradient_accumulation_steps=4,  # Result of experimentation\n",
    "    evaluation_strategy='no',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    report_to='none',\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# Initialize the HF Trainer\n",
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK_NvNnXugU_"
   },
   "source": [
    "#### Save the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrrTgT2-ui0g"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "szSX6m9kxZ2F"
   },
   "outputs": [],
   "source": [
    "# lora_model.save_pretrained(\"llama3_1_8b-lora-pretraining\")\n",
    "lora_model.save_pretrained(\"llama3_1_8b-lora-posttraining-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwbWWcx1xZ2F"
   },
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "- Load the saved PEFT model weights\n",
    "- Evaluate the performance of the trained PEFT model\n",
    "- Compare the results to the results from prior to fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS0D3uq4u_gC"
   },
   "source": [
    "### Load the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "24908a38adf64d9ba22ce7219d7b8e5a",
      "ef13435a14c6441c852c6c8b0a0a07e3",
      "7be6fefbf4f24e1b93dda47a4f8d7ce2",
      "d48330fa0b3943b3b0271698798928fd",
      "a8f95f154d7941d3ba4e7752ba2fb66d",
      "a3a04508f6134d8ea3769bfd382d9ed2",
      "1b6632e4f8224cea9a33f9b0883ca5f5",
      "cd6ab16af92c4a9aab8ee031803606fe",
      "92ba94294415477cb5cf1f4f5345ed3d",
      "d1cf5ed7af994bf7beffd305a640846b",
      "936c6f942db544a8a58421456a88fb96"
     ]
    },
    "id": "T_Rz7JQexZ2G",
    "outputId": "8956f0dc-a6dd-4899-ddf2-b821150ebc74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreating a new copy of Model meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24908a38adf64d9ba22ce7219d7b8e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "\n",
    "# Create a new, fresh copy of the Base foundational model\n",
    "\n",
    "print(f'Recreating a new copy of Model {model_name}')\n",
    "model_copy, tokenizer_copy = setup_model(model_name)\n",
    "\n",
    "\n",
    "\n",
    "# Load the LoRA model\n",
    "\n",
    "lora_weights_path = \"./llama3_1_8b-lora-posttraining-4\"\n",
    "config = PeftConfig.from_pretrained(lora_weights_path)\n",
    "\n",
    "lora_model_copy = PeftModel.from_pretrained(\n",
    "    model_copy,\n",
    "    lora_weights_path,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39kYmdM4eBcQ"
   },
   "source": [
    "### Evaluate its quality using our standardized evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "id": "exSCyAKrxZ2G",
    "outputId": "0c424d56-8113-4cbf-eb5e-4419765f5887"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:44<00:00,  3.21s/it]\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"evaluate_quality(test_dataset, lora_model_copy, tokenizer_copy)\",\n  \"rows\": 69,\n  \"fields\": [\n    {\n      \"column\": \"No.\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 20,\n        \"min\": 1,\n        \"max\": 69,\n        \"num_unique_values\": 69,\n        \"samples\": [\n          23,\n          1,\n          48\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abbrv Question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 69,\n        \"samples\": [\n          \" .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Dr . Adams and Dr . Jones informed Kate and Laura that they had retired and presented several options for future treatment .pronoun: they. A) dr . adams and dr . jones B) kate and laura\\nOutput:\",\n          \"e: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: freddy was upset with travis because the toaster he had bought from him didn't work .pronoun: he. A) freddy B) travis\\nOutput:\",\n          \"full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: George and Bob got free tickets to the play , but they gave them to Eric and Jeff , because they were particularly eager to see it .pronoun: they. A) george and bob B) eric and jeff\\nOutput:\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 66,\n        \"samples\": [\n          \"the large balls\",\n          \"susan and hannah\",\n          \"freddy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our Prediction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 63,\n        \"samples\": [\n          \"jeff\",\n          \"alonso\",\n          \"freddy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our ROUGE-1 F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31408249969485513,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 9,\n        \"samples\": [\n          0.33333333333333337,\n          0.3333333333333333,\n          0.6666666666666665\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our ROUGE-2 F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4642540952106733,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          1.0,\n          0.5714285714285715,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Our ROUGE-L F1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3161004345174067,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.3333333333333333,\n          0.6666666666666665,\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-480a62d3-fee0-424d-b6ae-205a773b84c4\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No.</th>\n",
       "      <th>Abbrv Question</th>\n",
       "      <th>Reference</th>\n",
       "      <th>Our Prediction</th>\n",
       "      <th>Our ROUGE-1 F1</th>\n",
       "      <th>Our ROUGE-2 F1</th>\n",
       "      <th>Our ROUGE-L F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>e: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: freddy was upset with travis because the toaster he had bought from him didn't work .pronoun: he. A) freddy B) travis\\nOutput:</td>\n",
       "      <td>freddy</td>\n",
       "      <td>freddy</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>te to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Fred covered his eye with his hand , because the wind was blowing sand around . He lowered it when the wind stopped .pronoun: it. A) his eye B) his hand\\nOutput:</td>\n",
       "      <td>his hand</td>\n",
       "      <td>his hand</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>ut: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: luke couldn't see the stage with adam in front of him because he is so short .pronoun: he. A) luke B) adam\\nOutput:</td>\n",
       "      <td>luke</td>\n",
       "      <td>luke</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>f-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: the board of aldermen refused the protesters a permit because they feared violence .pronoun: they. A) the board of aldermen B) the protesters\\nOutput:</td>\n",
       "      <td>the board of aldermen</td>\n",
       "      <td>the protesters</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: The man held the boy against his will .pronoun: his. A) the man B) the boy\\nOutput:</td>\n",
       "      <td>the boy</td>\n",
       "      <td>the boy</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>65</td>\n",
       "      <td>e Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Bill did not pass the ball to Steven although he was open .pronoun: he. A) bill B) steven\\nOutput:</td>\n",
       "      <td>steven</td>\n",
       "      <td>steven</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>66</td>\n",
       "      <td>to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Susan and Hannah knew that Ann and Polly's sons had been in car accidents , so they told them about it .pronoun: they. A) susan and hannah B) ann and polly\\nOutput:</td>\n",
       "      <td>susan and hannah</td>\n",
       "      <td>susan and hanna</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>aston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Peter did a lot worse than his good friend Jeff on the test because he had studied so hard .pronoun: he. A) peter B) jeff\\nOutput:</td>\n",
       "      <td>jeff</td>\n",
       "      <td>jeff</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>68</td>\n",
       "      <td>2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: I put the heavy books on the tables and they broke .pronoun: they. A) the heavy books B) the tables\\nOutput:</td>\n",
       "      <td>the tables</td>\n",
       "      <td>the tables</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>69</td>\n",
       "      <td>david's uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: kirk hired andy to take care of him .pronoun: him. A) kirk B) andy\\nOutput:</td>\n",
       "      <td>kirk</td>\n",
       "      <td>kirk</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 7 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-480a62d3-fee0-424d-b6ae-205a773b84c4')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-480a62d3-fee0-424d-b6ae-205a773b84c4 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-480a62d3-fee0-424d-b6ae-205a773b84c4');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-8ff5e39a-b4e5-4eee-9f7f-c1a1d1a9ad2c\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8ff5e39a-b4e5-4eee-9f7f-c1a1d1a9ad2c')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-8ff5e39a-b4e5-4eee-9f7f-c1a1d1a9ad2c button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "    No.  \\\n",
       "0     1   \n",
       "1     2   \n",
       "2     3   \n",
       "3     4   \n",
       "4     5   \n",
       "..  ...   \n",
       "64   65   \n",
       "65   66   \n",
       "66   67   \n",
       "67   68   \n",
       "68   69   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                          Abbrv Question  \\\n",
       "0      e: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: freddy was upset with travis because the toaster he had bought from him didn't work .pronoun: he. A) freddy B) travis\\nOutput:   \n",
       "1      te to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Fred covered his eye with his hand , because the wind was blowing sand around . He lowered it when the wind stopped .pronoun: it. A) his eye B) his hand\\nOutput:   \n",
       "2      ut: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: luke couldn't see the stage with adam in front of him because he is so short .pronoun: he. A) luke B) adam\\nOutput:   \n",
       "3      f-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: the board of aldermen refused the protesters a permit because they feared violence .pronoun: they. A) the board of aldermen B) the protesters\\nOutput:   \n",
       "4   uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: The man held the boy against his will .pronoun: his. A) the man B) the boy\\nOutput:   \n",
       "..                                                                                                                                                                                                                                                                                                                   ...   \n",
       "64    e Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Bill did not pass the ball to Steven although he was open .pronoun: he. A) bill B) steven\\nOutput:   \n",
       "65     to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Susan and Hannah knew that Ann and Polly's sons had been in car accidents , so they told them about it .pronoun: they. A) susan and hannah B) ann and polly\\nOutput:   \n",
       "66     aston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: Peter did a lot worse than his good friend Jeff on the test because he had studied so hard .pronoun: he. A) peter B) jeff\\nOutput:   \n",
       "67    2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: I put the heavy books on the tables and they broke .pronoun: they. A) the heavy books B) the tables\\nOutput:   \n",
       "68  david's uncles\\n\\nNegative Example 2 -\\nInput: sentence: gaston passed the half-empty plate to hershel because he was full .pronoun: he. A) Gaston B) hershel\\nOutput: hershel\\n\\nNow complete the following example -\\nInput: sentence: kirk hired andy to take care of him .pronoun: him. A) kirk B) andy\\nOutput:   \n",
       "\n",
       "                Reference   Our Prediction  Our ROUGE-1 F1  Our ROUGE-2 F1  \\\n",
       "0                  freddy           freddy        1.000000             0.0   \n",
       "1                his hand         his hand        1.000000             1.0   \n",
       "2                    luke             luke        1.000000             0.0   \n",
       "3   the board of aldermen   the protesters        0.333333             0.0   \n",
       "4                 the boy          the boy        1.000000             1.0   \n",
       "..                    ...              ...             ...             ...   \n",
       "64                 steven           steven        1.000000             0.0   \n",
       "65       susan and hannah  susan and hanna        0.666667             0.5   \n",
       "66                   jeff             jeff        1.000000             0.0   \n",
       "67             the tables       the tables        1.000000             1.0   \n",
       "68                   kirk             kirk        1.000000             0.0   \n",
       "\n",
       "    Our ROUGE-L F1  \n",
       "0         1.000000  \n",
       "1         1.000000  \n",
       "2         1.000000  \n",
       "3         0.333333  \n",
       "4         1.000000  \n",
       "..             ...  \n",
       "64        1.000000  \n",
       "65        0.666667  \n",
       "66        1.000000  \n",
       "67        1.000000  \n",
       "68        1.000000  \n",
       "\n",
       "[69 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average ROUGE Scores for Our Model:\n",
      "ROUGE-1 F1 Score: 0.7998\n",
      "ROUGE-2 F1 Score: 0.4068\n",
      "ROUGE-L F1 Score: 0.7961\n"
     ]
    }
   ],
   "source": [
    "evaluate_quality(test_dataset, lora_model_copy, tokenizer_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVW7DeBCvRS3"
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrGY503LvU74"
   },
   "source": [
    "With very little experimentation, or even selection of the layers for retraining, LoRA greatly improved the pronoun disambiguation problem classification quality, as tested on a testing data set, and measured by the the improved Rouge Scores:\n",
    "\n",
    "\n",
    "| Model           | Rouge-1 F1 Score Mean | Rouge-L F1 Score Mean |\n",
    "|-----------------|-----------------------|-----------------------|\n",
    "| Original        |                0.3619 |                0.3619 |\n",
    "| LoRA Fine-Tuned |                0.7998 |                0.7961 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ml-ud-cd13303-genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "07aef3fb740f4b83a8bb42435da8f0a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b3fc1514c5544ea900d9555b6216a49": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "10338e3da879478d81a2a14035efd1ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b6632e4f8224cea9a33f9b0883ca5f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "24908a38adf64d9ba22ce7219d7b8e5a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ef13435a14c6441c852c6c8b0a0a07e3",
       "IPY_MODEL_7be6fefbf4f24e1b93dda47a4f8d7ce2",
       "IPY_MODEL_d48330fa0b3943b3b0271698798928fd"
      ],
      "layout": "IPY_MODEL_a8f95f154d7941d3ba4e7752ba2fb66d"
     }
    },
    "367a36f7ed494312bfb4014900717580": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3eb81cd6d19d438e80de6b3faf101c5e",
      "placeholder": "​",
      "style": "IPY_MODEL_0b3fc1514c5544ea900d9555b6216a49",
      "value": " 4/4 [00:09&lt;00:00,  2.15s/it]"
     }
    },
    "3eb81cd6d19d438e80de6b3faf101c5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45d4f5e05ec0439294e7a7e298325d87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_10338e3da879478d81a2a14035efd1ce",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_817b866a3c2e41909343462ef4f016a3",
      "value": 4
     }
    },
    "61191cd3c85048e2a5d43911ae71273a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d6967496e9244a089ca6490ba4962307",
       "IPY_MODEL_45d4f5e05ec0439294e7a7e298325d87",
       "IPY_MODEL_367a36f7ed494312bfb4014900717580"
      ],
      "layout": "IPY_MODEL_d528a17a42744b60a5ac5e187624b488"
     }
    },
    "7be6fefbf4f24e1b93dda47a4f8d7ce2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd6ab16af92c4a9aab8ee031803606fe",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_92ba94294415477cb5cf1f4f5345ed3d",
      "value": 4
     }
    },
    "817b866a3c2e41909343462ef4f016a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "92ba94294415477cb5cf1f4f5345ed3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "936c6f942db544a8a58421456a88fb96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a3a04508f6134d8ea3769bfd382d9ed2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8f95f154d7941d3ba4e7752ba2fb66d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "adbef3527b3c4a28b49352332303253e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd6ab16af92c4a9aab8ee031803606fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d1cf5ed7af994bf7beffd305a640846b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d48330fa0b3943b3b0271698798928fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d1cf5ed7af994bf7beffd305a640846b",
      "placeholder": "​",
      "style": "IPY_MODEL_936c6f942db544a8a58421456a88fb96",
      "value": " 4/4 [00:09&lt;00:00,  2.18s/it]"
     }
    },
    "d528a17a42744b60a5ac5e187624b488": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d6967496e9244a089ca6490ba4962307": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adbef3527b3c4a28b49352332303253e",
      "placeholder": "​",
      "style": "IPY_MODEL_07aef3fb740f4b83a8bb42435da8f0a5",
      "value": "Loading checkpoint shards: 100%"
     }
    },
    "ef13435a14c6441c852c6c8b0a0a07e3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a3a04508f6134d8ea3769bfd382d9ed2",
      "placeholder": "​",
      "style": "IPY_MODEL_1b6632e4f8224cea9a33f9b0883ca5f5",
      "value": "Loading checkpoint shards: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
